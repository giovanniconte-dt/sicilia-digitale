{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Chat Base con LLM Locale\n",
    "\n",
    "**Obiettivo**: Creare la prima chat funzionante usando LangChain e Ollama\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import e Setup\n",
    "\n",
    "Importiamo le librerie necessarie.\n",
    "\n",
    "**Nota**: Assicurati che Ollama sia in esecuzione:\n",
    "- **Docker**: `docker-compose up -d` (porta 11434)\n",
    "- **Installazione diretta**: `ollama serve` (porta 11434)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import LangChain (versione 1.0+)\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "print(\"‚úÖ Import completati\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connessione a Ollama\n",
    "\n",
    "Creiamo una connessione al modello LLM locale tramite Ollama.\n",
    "\n",
    "**Nota**: Assicurati che Ollama sia in esecuzione (`ollama serve`)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Crea istanza LLM collegata a Ollama (API moderna LangChain 1.0+)\n",
    "# Modello: llama3.2:3b (o il modello che hai scaricato)\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2:1b\",  # Cambia se usi un modello diverso\n",
    "    base_url=\"http://localhost:11434\"  # URL default Ollama\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM inizializzato\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prima Chat Semplice\n",
    "\n",
    "Facciamo la prima chiamata al LLM!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prima chiamata semplice\n",
    "prompt = \"Ciao! Presentati brevemente.\"\n",
    "\n",
    "print(f\"Domanda: {prompt}\")\n",
    "print(\"\\nRisposta:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat Interattiva Base\n",
    "\n",
    "Creiamo una funzione per chattare in modo interattivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def chat_semplice(messaggio):\n",
    "    \"\"\"Funzione per chat semplice senza memoria\"\"\"\n",
    "    risposta = llm.invoke(messaggio)\n",
    "    return risposta\n",
    "\n",
    "# Test\n",
    "domanda = \"Quali sono i vantaggi di usare un chatbot nella pubblica amministrazione?\"\n",
    "risposta = chat_semplice(domanda)\n",
    "\n",
    "print(f\"Domanda: {domanda}\")\n",
    "print(f\"\\nRisposta:\\n{risposta}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Esplorazione Parametri LLM\n",
    "\n",
    "### **temperature** (0.0 - 1.0)\n",
    "Controlla la **creativit√†** dell'output.\n",
    "\n",
    "- **0.0-0.3**: Output deterministico e prevedibile. Ideale per FAQ e risposte precise.\n",
    "- **0.4-0.7**: Bilanciamento tra coerenza e variet√†. Uso generale conversazionale.\n",
    "- **0.8-1.0**: Output creativo e imprevedibile. Utile per storytelling e brainstorming.\n",
    "\n",
    "**Default**: 0.8\n",
    "\n",
    "### **top_p** (0.0 - 1.0) - Nucleus Sampling\n",
    "Definisce il **sottoinsieme dinamico di token** da cui scegliere.\n",
    "\n",
    "- **0.7-0.8**: Output focalizzato, considera solo i token pi√π probabili.\n",
    "- **0.9-0.95**: Output pi√π diversificato, include pi√π opzioni.\n",
    "\n",
    "**Best practice**: Usare **o** temperature **o** top_p, non entrambi insieme.\n",
    "\n",
    "### **num_predict** (numero intero)\n",
    "**Numero massimo di token** generati.\n",
    "\n",
    "- **100-200**: Risposte brevi\n",
    "- **500-1000**: Risposte articolate (valore consigliato)\n",
    "- **>1000**: Contenuti lunghi\n",
    "\n",
    "### **repeat_penalty** (1.0 - 2.0)\n",
    "Penalizza le **ripetizioni** di parole gi√† generate.\n",
    "\n",
    "- **1.0**: Nessuna penalit√†\n",
    "- **1.1-1.3**: Penalit√† leggera, riduce ripetizioni mantenendo naturalezza\n",
    "- **>1.5**: Penalit√† forte, pu√≤ rendere il testo meno naturale\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# LLM con parametri personalizzati\n",
    "llm_configurato = OllamaLLM(\n",
    "    model=\"llama3.2:1b\",\n",
    "    temperature=0.7,  # Creativit√†: 0.0 (deterministico) - 1.0 (creativo)\n",
    "    # top_p=1.0,  # Nucleus sampling\n",
    "    num_predict=500,  # Max token generati\n",
    "    repeat_penalty=1.1  # Penalit√† ripetizioni\n",
    ")\n",
    "\n",
    "prompt = \"Spiega in 2 righe cosa sono i Large Language Models.\"\n",
    "# prompt = \"Inventa un nome originale per una nuova bevanda energetica futuristica e scrivi il suo slogan.\"\n",
    "\n",
    "risposta = llm_configurato.invoke(prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nRisposta:\\n{risposta}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Esempio Pratico: Chatbot FAQ\n",
    "\n",
    "Creiamo un esempio pi√π realistico: un chatbot che risponde a domande frequenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def chatbot_faq(domanda):\n",
    "    \"\"\"Chatbot semplice per FAQ\"\"\"\n",
    "    # Aggiungiamo contesto al prompt\n",
    "    prompt_completo = f\"\"\"\n",
    "Sei un assistente virtuale di Sicilia Digitale, azienda che fornisce servizi ICT per la Pubblica Amministrazione.\n",
    "\n",
    "Rispondi in modo professionale, chiaro e conciso alle domande degli utenti.\n",
    "Se non conosci la risposta, ammettilo onestamente.\n",
    "\n",
    "Domanda: {domanda}\n",
    "\n",
    "Risposta:\n",
    "\"\"\"\n",
    "    \n",
    "    risposta = llm.invoke(prompt_completo)\n",
    "    return risposta.strip()\n",
    "\n",
    "# Test con domande reali\n",
    "domande_test = [\n",
    "    \"Quali servizi offre Sicilia Digitale?\",\n",
    "    \"Come posso contattare il supporto?\",\n",
    "    \"Quali sono gli orari di apertura?\",\n",
    "    \"Qual √® la partita IVA o il codice fiscale dell'azienda?\"\n",
    "]\n",
    "\n",
    "for domanda in domande_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Domanda: {domanda}\")\n",
    "    print(f\"\\nRisposta:\")\n",
    "    risposta = chatbot_faq(domanda)\n",
    "    print(risposta)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# Successivamente, sottoporremo le stesse domande ad un agente con knowledge base"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Note e Best Practices\n",
    "\n",
    "### Cosa abbiamo imparato:\n",
    "1. Come connettere LangChain a Ollama\n",
    "2. Come fare chiamate base a un LLM\n",
    "3. Come controllare parametri (temperature, etc.)\n",
    "4. Come strutturare prompt con contesto\n",
    "\n",
    "### Limitazioni attuali:\n",
    "- Nessuna memoria: ogni chiamata √® indipendente\n",
    "- Nessun retrieval: il LLM usa solo conoscenza pre-addestrata\n",
    "- Nessun tool: non pu√≤ interagire con sistemi esterni\n",
    "\n",
    "### Prossimi passi:\n",
    "- Aggiungeremo memoria conversazionale\n",
    "- Implementeremo RAG per knowledge base\n",
    "- Integreremo tools per azioni esterne\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulazioni! Hai completato il Notebook 1! üéâ**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

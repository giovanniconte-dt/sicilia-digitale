{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Prompt Templates e Variabili\n",
    "\n",
    "**Obiettivo**: Imparare a creare e usare prompt templates con LangChain per prompt riutilizzabili e dinamici\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Inizializza LLM\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")\n",
    "\n",
    "print(\"âœ… Setup completato\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Template Base\n",
    "\n",
    "Un prompt template permette di definire un prompt con variabili che possono essere sostituite.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Crea un prompt template semplice\n",
    "template = \"\"\"\n",
    "Sei un assistente esperto. Rispondi alla seguente domanda:\n",
    "\n",
    "Domanda: {domanda}\n",
    "\n",
    "Risposta:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"domanda\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Usa il template con una variabile\n",
    "prompt_formattato = prompt_template.format(domanda=\"Cos'Ã¨ un LLM?\")\n",
    "\n",
    "print(\"Prompt formattato:\")\n",
    "print(prompt_formattato)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Esegui con LLM\n",
    "risposta = llm.invoke(prompt_formattato)\n",
    "print(\"\\nRisposta:\")\n",
    "print(risposta)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Template con Multiple Variabili\n",
    "\n",
    "Possiamo usare piÃ¹ variabili in un template.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Template con piÃ¹ variabili\n",
    "template_multi = \"\"\"\n",
    "Sei un assistente virtuale di {azienda}, specializzato in {settore}.\n",
    "\n",
    "L'utente ti chiede: {domanda}\n",
    "\n",
    "Rispondi in modo professionale e specifico per il settore {settore}.\n",
    "\n",
    "Risposta:\n",
    "\"\"\"\n",
    "\n",
    "prompt_multi = PromptTemplate(\n",
    "    input_variables=[\"azienda\", \"settore\", \"domanda\"],\n",
    "    template=template_multi\n",
    ")\n",
    "\n",
    "# Formatta con valori\n",
    "prompt_formattato = prompt_multi.format(\n",
    "    azienda=\"Sicilia Digitale\",\n",
    "    settore=\"servizi ICT per la Pubblica Amministrazione\",\n",
    "    domanda=\"Come posso richiedere un servizio?\"\n",
    ")\n",
    "\n",
    "print(\"Prompt formattato:\")\n",
    "print(prompt_formattato)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "risposta = llm.invoke(prompt_formattato)\n",
    "print(\"\\nRisposta:\")\n",
    "print(risposta)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain: Template + LLM + Parser\n",
    "\n",
    "LangChain permette di creare \"chain\" che combinano piÃ¹ componenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Crea chain completa\n",
    "template_chain = \"\"\"\n",
    "Sei un assistente che risponde a domande tecniche.\n",
    "\n",
    "Domanda: {domanda}\n",
    "\n",
    "Fornisci una risposta chiara e concisa (max 100 parole).\n",
    "\n",
    "Risposta:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"domanda\"],\n",
    "    template=template_chain\n",
    ")\n",
    "\n",
    "# Crea chain: prompt â†’ LLM â†’ parser\n",
    "chain = (\n",
    "    {\"domanda\": RunnablePassthrough()}  # Passa input come dict\n",
    "    | prompt_template  # Applica template\n",
    "    | llm  # Esegui LLM\n",
    "    | StrOutputParser()  # Parse output come stringa\n",
    ")\n",
    "\n",
    "# Usa la chain\n",
    "risposta = chain.invoke(\"Cos'Ã¨ LangChain?\")\n",
    "\n",
    "print(\"Domanda: Cos'Ã¨ LangChain?\")\n",
    "print(f\"\\nRisposta:\\n{risposta}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Template per Chatbot Assistenza\n",
    "\n",
    "Creiamo un template piÃ¹ complesso per un chatbot di assistenza.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Template per chatbot assistenza\n",
    "template_assistenza = \"\"\"\n",
    "Sei un assistente virtuale di {azienda}, azienda che fornisce {servizi}.\n",
    "\n",
    "Il tuo ruolo Ã¨:\n",
    "- Rispondere alle domande degli utenti in modo chiaro e professionale\n",
    "- Fornire informazioni accurate sui servizi\n",
    "- Se non conosci la risposta, suggerisci di contattare il supporto\n",
    "\n",
    "Contesto aziendale:\n",
    "{contesto_aziendale}\n",
    "\n",
    "Domanda dell'utente: {domanda_utente}\n",
    "\n",
    "Risposta:\n",
    "\"\"\"\n",
    "\n",
    "prompt_assistenza = PromptTemplate(\n",
    "    input_variables=[\"azienda\", \"servizi\", \"contesto_aziendale\", \"domanda_utente\"],\n",
    "    template=template_assistenza\n",
    ")\n",
    "\n",
    "# Dati aziendali\n",
    "contesto_sicilia_digitale = \"\"\"\n",
    "Sicilia Digitale S.p.A. Ã¨ un'azienda che fornisce servizi ICT per la Pubblica Amministrazione.\n",
    "Offre soluzioni per digitalizzazione, cloud, sicurezza informatica e assistenza tecnica.\n",
    "Opera principalmente in Sicilia ma serve anche altre regioni.\n",
    "\"\"\"\n",
    "\n",
    "# Formatta e usa\n",
    "prompt_formattato = prompt_assistenza.format(\n",
    "    azienda=\"Sicilia Digitale\",\n",
    "    servizi=\"servizi ICT per la Pubblica Amministrazione\",\n",
    "    contesto_aziendale=contesto_sicilia_digitale,\n",
    "    domanda_utente=\"Quali servizi di cloud computing offrite?\"\n",
    ")\n",
    "\n",
    "print(\"Prompt completo:\")\n",
    "print(prompt_formattato)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "risposta = llm.invoke(prompt_formattato)\n",
    "print(\"\\nRisposta:\")\n",
    "print(risposta)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Note e Prossimi Passi\n",
    "\n",
    "### Cosa abbiamo imparato:\n",
    "1. Come creare prompt templates riutilizzabili\n",
    "2. Come usare variabili nei template\n",
    "3. Come creare chain con template + LLM\n",
    "4. Best practices per strutturare prompt\n",
    "\n",
    "### Prossimi passi:\n",
    "- Aggiungeremo memoria conversazionale\n",
    "- Useremo template con RAG per knowledge base\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulazioni! Hai completato il Notebook 2! ðŸŽ‰**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Memoria Conversazionale con LCEL\n",
    "\n",
    "**Obiettivo**: Implementare memoria breve e a lungo termine per un chatbot con LangChain 1.0+ usando LCEL (LangChain Expression Language)\n",
    "\n",
    "**Nota**: Questo notebook usa l'API moderna LCEL di LangChain 1.0+, non le API legacy (ConversationChain, ConversationBufferMemory).\n",
    "\n",
    "---\n",
    "Questo snippet implementa un chatbot conversazionale con memoria usando LangChain e Ollama.\n",
    "Il codice crea un'istanza di ChatOllama che si connette al modello locale llama3.2:3b, poi utilizza un dizionario Python (store) per memorizzare le cronologie delle conversazioni separate per session_id.\n",
    "\n",
    "La funzione get_session_history gestisce il recupero o la creazione di nuove cronologie in memoria.\n",
    "Il wrapper RunnableWithMessageHistory integra automaticamente la gestione della memoria nel modello, permettendo di mantenere il contesto conversazionale tra chiamate successive.\n",
    "\n",
    "Nell'esempio pratico, il chatbot ricorda il nome dell'utente nella seconda domanda perch√© entrambe le interazioni utilizzano lo stesso session_id (\"abcd\"), consentendo al modello di accedere alla cronologia completa della conversazione memorizzata in memoria."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T08:51:59.009793424Z",
     "start_time": "2026-01-27T08:51:50.049752665Z"
    }
   },
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Usa ChatOllama direttamente senza prompt template custom\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.1)\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrappa direttamente il modello\n",
    "chat = RunnableWithMessageHistory(\n",
    "    llm,\n",
    "    get_session_history\n",
    ")\n",
    "\n",
    "# Usa\n",
    "response = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Ciao, mi chiamo Giovanni\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"abcd\"}}\n",
    ")\n",
    "print(response.content)\n",
    "\n",
    "response = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Qual √® il mio nome?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"abcd\"}}\n",
    ")\n",
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao Giovanni! Piacere di conoscerti. Come posso aiutarti oggi? Vuoi parlare di qualcosa in particolare o vuoi semplicemente chiacchierare un po'? Sono qui per ascoltarti e aiutarti in ogni modo possibile.\n",
      "Mi dispiace, Giovanni! Mi sembra che ci sia stato un piccolo errore. Il tuo nome √® stato scritto come \"Giovanni\" all'inizio della nostra conversazione, ma poi hai detto \"mi chiamo Giovanni\". Se vuoi, possiamo iniziare da capo e scoprire il tuo vero nome!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ChatMessageHistory - Memoria Breve Termine\n",
    "\n",
    "In LCEL, usiamo `InMemoryChatMessageHistory` per memorizzare tutti i messaggi della conversazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T08:53:30.394479839Z",
     "start_time": "2026-01-27T08:52:58.177159823Z"
    }
   },
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 1. Inizializza il modello LLM locale\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.1)\n",
    "\n",
    "# 2. Crea lo store per memorizzare le sessioni\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Recupera o crea una nuova cronologia per la sessione\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 3. Crea il chatbot con memoria\n",
    "chat = RunnableWithMessageHistory(\n",
    "    llm,\n",
    "    get_session_history\n",
    ")\n",
    "\n",
    "# 4. SESSIONE STUDENTE 1\n",
    "print(\"=== CONVERSAZIONE STUDENTE 1 ===\")\n",
    "response1 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Ciao, mi chiamo Luca e studio a Milano\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_1\"}}\n",
    ")\n",
    "print(f\"AI: {response1.content}\\n\")\n",
    "\n",
    "response2 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Dove studio io?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_1\"}}\n",
    ")\n",
    "print(f\"AI: {response2.content}\\n\")\n",
    "\n",
    "# 5. SESSIONE STUDENTE 2 (sessione diversa, memoria separata)\n",
    "print(\"=== CONVERSAZIONE STUDENTE 2 ===\")\n",
    "response3 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Ciao, mi chiamo Sara e studio a Roma\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_2\"}}\n",
    ")\n",
    "print(f\"AI: {response3.content}\\n\")\n",
    "\n",
    "response4 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Dove studio io?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_2\"}}\n",
    ")\n",
    "print(f\"AI: {response4.content}\\n\")\n",
    "\n",
    "# 6. Torniamo alla SESSIONE STUDENTE 1\n",
    "print(\"=== TORNIAMO ALLO STUDENTE 1 ===\")\n",
    "response5 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Come mi chiamo?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_1\"}}\n",
    ")\n",
    "print(f\"AI: {response5.content}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSAZIONE STUDENTE 1 ===\n",
      "AI: Ciao Luca! Sono felice di conoscerti. Sono un AI, quindi non ho una presenza fisica, ma sono qui per aiutarti in qualsiasi modo possa. Come stai? La vita a Milano √® fantastica? Qual √® il tuo campo di studio?\n",
      "\n",
      "AI: Mi scuso per la confusione! Non ho saputo che tu avessi detto dove studi. Se vuoi, puoi raccontarmi dove studii a Milano. Sono qui per ascoltarti e aiutarti in qualsiasi modo possa!\n",
      "\n",
      "=== CONVERSAZIONE STUDENTE 2 ===\n",
      "AI: Ciao Sara! Sono felice di conoscerti. Sono qui per aiutarti in qualsiasi modo possa. Come stai? La vita a Roma √® fantastica, no? Qual √® il tuo campo di studio? E come ti piace la citt√†?\n",
      "\n",
      "AI: Mi scuso per non averlo chiesto prima! Non ho saputo dove studassi. Sei a Roma? Quale universit√† frequenti? Sono curiosa di saperne di pi√π sulla tua vita studentesca!\n",
      "\n",
      "=== TORNIAMO ALLO STUDENTE 1 ===\n",
      "AI: Mi dispiace averlo dimenticato! Ti ho detto che ti chiamavi Luca, ma non avevo saputo dove studi. Se vuoi, puoi raccontarmi dove studi a Milano e come si chiama la tua universit√† o facolt√†. Sono qui per ascoltarti!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chat con Memoria Buffer usando LCEL\n",
    "\n",
    "Questo pattern rappresenta l'implementazione production-ready di un chatbot conversazionale con memoria utilizzando l'architettura LCEL (LangChain Expression Language). √à il metodo standard per costruire assistenti AI professionali con gestione della cronologia conversazionale.\n",
    "\n",
    "### Architettura a Tre Livelli\n",
    "Il codice separa chiaramente tre responsabilit√†. Il ChatPromptTemplate definisce la struttura dell'interazione, includendo un messaggio di sistema per configurare il comportamento dell'assistente, un MessagesPlaceholder per iniettare dinamicamente la cronologia conversazionale e un template per l'input utente. La chain LCEL (prompt | llm) crea una pipeline che processa sequenzialmente il prompt template e lo invia al modello. Il wrapper RunnableWithMessageHistory orchestra automaticamente il recupero della cronologia, l'integrazione nel prompt e il salvataggio dei nuovi messaggi.\n",
    "\n",
    "### Gestione Avanzata degli Input\n",
    "I parametri input_messages_key e history_messages_key sono necessari perch√© il runnable accetta un dizionario come input anzich√© una semplice lista. Questi specificano dove mappare l'input dell'utente e dove iniettare la cronologia nel template, consentendo a LangChain di gestire automaticamente il flusso dei dati tra memoria e modello.\n",
    "\n",
    "### Vantaggi del Pattern\n",
    "Questo approccio garantisce modularit√† e manutenibilit√†. Puoi modificare il comportamento del sistema cambiando solo il prompt template, aggiungere preprocessing/postprocessing estendendo la chain, o sostituire il backend di memoria senza riscrivere la logica applicativa. Il sistema supporta nativamente sessioni multiple isolate attraverso session_id, rendendolo ideale per applicazioni multi-utente.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Crea prompt template con LCEL (ChatPromptTemplate)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sei un assistente amichevole. Rispondi alle domande dell'utente.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Placeholder per la storia\n",
    "    (\"human\", \"{input}\")  # Input dell'utente\n",
    "])\n",
    "\n",
    "# Crea chain LCEL: prompt | llm\n",
    "chain = prompt | llm\n",
    "\n",
    "# Store per gestire memoria per diverse sessioni\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    \"\"\"Funzione per ottenere/creare memoria per una sessione\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Crea RunnableWithMessageHistory (gestisce automaticamente la memoria)\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Test conversazione (LCEL usa invoke con config)\n",
    "session_id = \"test_session\"\n",
    "\n",
    "print(\"=== Conversazione 1 ===\")\n",
    "risposta1 = conversation.invoke(\n",
    "    {\"input\": \"Ciao, mi chiamo Mario\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Risposta: {risposta1.content}\\n\")\n",
    "\n",
    "print(\"=== Conversazione 2 ===\")\n",
    "risposta2 = conversation.invoke(\n",
    "    {\"input\": \"Qual √® il mio nome?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Risposta: {risposta2.content}\\n\")\n",
    "\n",
    "print(\"=== Conversazione 3 ===\")\n",
    "risposta3 = conversation.invoke(\n",
    "    {\"input\": \"Dove lavoro?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Risposta: {risposta3.content}\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flusso di Esecuzione\n",
    "Quando si invoca `conversation.invoke({\"input\": \"Ciao, mi chiamo Mario\"}, config={\"configurable\": {\"session_id\": \"test_session\"}})`, succede questo in sequenza:\n",
    "  - LangChain chiama get_session_history(\"test_session\") per recuperare la cronologia\n",
    "  - sostituisce {input} nel template con \"Ciao, mi chiamo Mario\", inserisce i messaggi storici nel MessagesPlaceholder\n",
    "  - invia tutto al modello tramite la chain\n",
    "  - salva automaticamente sia il messaggio utente che la risposta AI nella memoria.\n",
    "\n",
    "\n",
    "\n",
    "Nella seconda invocazione, quando chiedi \"Qual √® il mio nome?\", il sistema recupera la stessa cronologia contenente il messaggio precedente, quindi il modello pu√≤ rispondere correttamente \"Mario\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat con Summary Memory\n",
    "\n",
    "Usiamo summary memory in una conversazione lunga.\n",
    "\n",
    "Questo codice implementa un sistema di summary memory incrementale che aggiorna progressivamente il riassunto della conversazione invece di ricrearlo da zero ogni volta.\n",
    "\n",
    "### Logica del Summary Incrementale\n",
    "Quando la conversazione supera 6 messaggi, il sistema recupera il summary precedente da `summary_text_store` e lo combina con i nuovi messaggi da archiviare.\n",
    "Il prompt di aggiornamento chiede esplicitamente all'LLM di \"combinare le vecchie info con le nuove\", mantenendo i dati storici (nome, citt√† iniziali) e aggiungendo i nuovi dettagli (hobby, interessi emersi dopo).\n",
    "\n",
    "### Vantaggi dell'Approccio\n",
    "Questo metodo preserva meglio le informazioni iniziali attraverso molteplici cicli di summarization. Il summary diventa cumulativo: ogni volta che viene aggiornato, porta con s√© i fatti delle conversazioni precedenti.\n",
    "\n",
    "### Flusso di Esecuzione\n",
    "Il sistema conta i messaggi regolari escludendo il messaggio [CONTEXT] che contiene il summary. Quando supera il limite, mantiene gli ultimi 2 messaggi recenti per continuit√† e invia all'LLM sia il vecchio summary che i nuovi messaggi da archiviare. L'LLM genera un summary aggiornato che viene salvato in `summary_text_store` e iniettato come primo HumanMessage nella nuova cronologia.\n",
    "\n",
    "La temperature ridotta a 0.1 garantisce risposte pi√π deterministiche e accurate nel recupero delle informazioni.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:00:17.643826237Z",
     "start_time": "2026-01-27T08:57:42.814491440Z"
    }
   },
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Inizializza LLM\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.1)\n",
    "\n",
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Sei un assistente amichevole.\n",
    "IMPORTANTE: Leggi attentamente TUTTA la cronologia.\n",
    "Se c'√® un messaggio di [CONTEXT] o [RIASSUNTO], usalo come verit√† assoluta sull'utente.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# Store\n",
    "summary_store = {}\n",
    "summary_text_store = {}\n",
    "\n",
    "def get_summary_history(session_id: str, max_messages=6) -> BaseChatMessageHistory:\n",
    "    \"\"\"Crea/recupera cronologia con summary INCREMENTALE\"\"\"\n",
    "\n",
    "    if session_id not in summary_store:\n",
    "        summary_store[session_id] = InMemoryChatMessageHistory()\n",
    "        summary_text_store[session_id] = \"\" # Inizializza stringa vuota\n",
    "\n",
    "    history = summary_store[session_id]\n",
    "\n",
    "    # 1. Identifica i messaggi \"veri\" (escludendo il messaggio tecnico di context se presente)\n",
    "    regular_messages = [m for m in history.messages\n",
    "                       if not (isinstance(m, HumanMessage) and m.content.startswith(\"[CONTEXT]\"))]\n",
    "\n",
    "    # 2. Controllo Limite\n",
    "    if len(regular_messages) > max_messages:\n",
    "        print(f\"\\n‚ö†Ô∏è LIMITE SUPERATO ({len(regular_messages)} messaggi). Aggiorno il summary...\\n\")\n",
    "\n",
    "        # Separiamo messaggi da archiviare (old) e messaggi da tenere (recent)\n",
    "        # Teniamo gli ultimi 2 per mantenere il flusso della conversazione fluido\n",
    "        messages_to_summarize = regular_messages[:-2]\n",
    "        recent_messages = regular_messages[-2:]\n",
    "\n",
    "        # Recuperiamo il VECCHIO summary (se esiste)\n",
    "        previous_summary = summary_text_store.get(session_id, \"\")\n",
    "\n",
    "        # Convertiamo i messaggi da archiviare in testo\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{'UTENTE' if isinstance(m, HumanMessage) else 'AI'}: {m.content}\"\n",
    "            for m in messages_to_summarize\n",
    "        ])\n",
    "\n",
    "        # 3. Prompt di AGGIORNAMENTO (Cruciale: Unisce Vecchio + Nuovo)\n",
    "        update_prompt = f\"\"\"Sei un gestore di memoria.\n",
    "Ecco le informazioni che gi√† conosci sull'utente:\n",
    "{previous_summary}\n",
    "\n",
    "Ecco la nuova conversazione appena avvenuta:\n",
    "{conversation_text}\n",
    "\n",
    "COMPITO:\n",
    "Aggiorna il profilo utente combinando le vecchie info con le nuove.\n",
    "Mantieni nome, citt√†, lavoro e aggiungi nuovi interessi o dettagli.\n",
    "Sii sintetico. Rispondi SOLO con la lista dei fatti aggiornata.\n",
    "\"\"\"\n",
    "\n",
    "        # Genera il nuovo summary\n",
    "        summary_response = llm.invoke([{\"role\": \"user\", \"content\": update_prompt}])\n",
    "        new_summary_text = summary_response.content.strip()\n",
    "\n",
    "        # Salviamo il testo aggiornato\n",
    "        summary_text_store[session_id] = new_summary_text\n",
    "        print(f\"üìù Summary Aggiornato: {new_summary_text}\\n\")\n",
    "\n",
    "        # 4. Ricostruzione della History\n",
    "        new_history = InMemoryChatMessageHistory()\n",
    "\n",
    "        # Iniettiamo il summary come PRIMO messaggio (chiaro per l'LLM)\n",
    "        # Usiamo un formato esplicito\n",
    "        context_message = HumanMessage(content=f\"[CONTEXT] RIEPILOGO DATI UTENTE:\\n{new_summary_text}\")\n",
    "        new_history.add_message(context_message)\n",
    "\n",
    "        # Reinseriamo gli ultimi messaggi per non perdere il filo immediato\n",
    "        for msg in recent_messages:\n",
    "            new_history.add_message(msg)\n",
    "\n",
    "        # Sostituiamo la history nello store\n",
    "        summary_store[session_id] = new_history\n",
    "        return new_history\n",
    "\n",
    "    return history\n",
    "\n",
    "# Setup Runnable\n",
    "conversation_summary = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_summary_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# --- TEST ---\n",
    "print(\"=== TEST SUMMARY INCREMENTALE ===\\n\")\n",
    "session_id = \"test_incrementale_v2\"\n",
    "\n",
    "# Simuliamo una conversazione lunga per forzare il summary\n",
    "messaggi = [\n",
    "    \"Ciao, mi chiamo Mario.\",                       # 1\n",
    "    \"Lavoro a Palermo come sviluppatore.\",          # 2\n",
    "    \"Ho 5 anni di esperienza in Python.\",           # 3\n",
    "    \"Sto studiando LangChain.\",                     # 4\n",
    "    \"Mi piace cucinare la pasta.\",                  # 5\n",
    "    \"Voglio fare un chatbot.\",                      # 6\n",
    "    \"Mi piace anche il calcio.\",                    # 7\n",
    "    \"A che punto siamo?\",                           # 8\n",
    "    \"Come mi chiamo e dove lavoro?\",                # 9\n",
    "    \"Qual √® il mio framework preferito?\",           # 10\n",
    "]\n",
    "\n",
    "# messaggi = [\n",
    "#     \"Ciao, mi chiamo Giulia.\",\n",
    "#     \"Attualmente abito a Milano.\",\n",
    "#     \"Lavoro come Data Analyst in una banca.\",\n",
    "#     \"Uso Python e SQL tutti i giorni per lavoro.\",\n",
    "#     \"Nel tempo libero mi piace molto fare giardinaggio.\",\n",
    "#     \"Ho un cane di nome Rex.\",\n",
    "#     \"Il mio piatto preferito √® la carbonara.\",\n",
    "#     \"Vorrei imparare a usare LangChain per automatizzare dei report.\",\n",
    "#     \"Qual √® la differenza principale tra LangChain e LlamaIndex?\",\n",
    "#     \"Sto avendo qualche difficolt√† a capire come funzionano gli agenti.\",\n",
    "#     \"Ieri sono andata al cinema a vedere un film di fantascienza.\",\n",
    "#     \"Tra l'altro, il mio colore preferito √® il verde.\",\n",
    "#     \"Ho una notizia: tra un mese mi trasferir√≤ a Torino.\",\n",
    "#     \"L√¨ continuer√≤ a lavorare ma in smart working.\",\n",
    "#     \"Vorrei anche iscrivermi a un corso di tennis.\",\n",
    "#     \"Non bevo caff√®, preferisco il t√® matcha.\",\n",
    "#     \"Mi consigli un buon libro tecnico sull'AI?\",\n",
    "#     \"Sto provando a far girare un modello locale sul mio PC.\",\n",
    "#     \"Senti, facciamo un punto della situazione.\",\n",
    "#     \"Ti ricordi come mi chiamo, dove vivo ora e dove andr√≤ a vivere tra poco?\"\n",
    "# ]\n",
    "\n",
    "for i, msg in enumerate(messaggi, 1):\n",
    "    print(f\"--- Turno {i} ---\")\n",
    "    print(f\"üë§ {msg}\")\n",
    "\n",
    "    risposta = conversation_summary.invoke(\n",
    "        {\"input\": msg},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    print(f\"ü§ñ {risposta.content}\\n\")\n",
    "\n",
    "    # Debug Memoria\n",
    "    history = summary_store[session_id].messages\n",
    "    if len(history) > 0 and \"[CONTEXT]\" in history[0].content:\n",
    "        print(f\"üîç Contenuto Memoria (Primo Messaggio): {history[0].content}...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SUMMARY INCREMENTALE ===\n",
      "\n",
      "--- Turno 1 ---\n",
      "üë§ Ciao, mi chiamo Mario.\n",
      "ü§ñ Ciao Mario! Sono felice di conoscerti. Come posso aiutarti oggi?\n",
      "\n",
      "--- Turno 2 ---\n",
      "üë§ Lavoro a Palermo come sviluppatore.\n",
      "ü§ñ Ciao Mario! Sembra che tu sia un esperto nel settore dello sviluppo, lavorando a Palermo. Quale tipo di progetti stai lavorando attualmente? Ecco, posso aiutarti con qualsiasi domanda o problema che possa avere.\n",
      "\n",
      "--- Turno 3 ---\n",
      "üë§ Ho 5 anni di esperienza in Python.\n",
      "ü§ñ Hai una solida base nel linguaggio Python! Con 5 anni di esperienza, sei probabilmente stato impegnato su vari progetti e hai sviluppato abilit√† significative nella programmazione. Come stai utilizzando la tua conoscenza di Python nel tuo lavoro a Palermo?\n",
      "\n",
      "--- Turno 4 ---\n",
      "üë§ Sto studiando LangChain.\n",
      "ü§ñ LangChain √® un framework molto interessante per l'intelligenza artificiale e la programmazione linguaggio naturale! Stai probabilmente esplorando le sue funzionalit√† per sviluppare applicazioni pi√π sofisticate.\n",
      "\n",
      "Come stai avvicinando LangChain nel tuo studio? Stai lavorando su progetti specifici o stai cercando di comprendere meglio le sue basi teoriche?\n",
      "\n",
      "--- Turno 5 ---\n",
      "üë§ Mi piace cucinare la pasta.\n",
      "\n",
      "‚ö†Ô∏è LIMITE SUPERATO (8 messaggi). Aggiorno il summary...\n",
      "\n",
      "üìù Summary Aggiornato: * Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)\n",
      "\n",
      "ü§ñ La pasta √® un classico! Cucinare la pasta pu√≤ essere una grande fonte di gioia e relax, soprattutto quando si crea qualcosa di nuovo e delizioso.\n",
      "\n",
      "Ecco, mentre studi LangChain, forse potresti trovare ispirazione nella creativit√† culinaria? La programmazione linguaggio naturale e l'intelligenza artificiale possono essere molto simili alle ricette e ai processi di cucina...\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)...\n",
      "--- Turno 6 ---\n",
      "üë§ Voglio fare un chatbot.\n",
      "ü§ñ Un chatbot! Sembra un progetto eccitante!\n",
      "\n",
      "LangChain √® una biblioteca molto utile per creare chatbot, in quanto offre strumenti e funzionalit√† per la programmazione linguaggio naturale e l'intelligenza artificiale.\n",
      "\n",
      "Qual √® il tuo obiettivo con il chatbot? Vuoi creare un chatbot per aiutare le persone a risolvere problemi specifici o semplicemente per divertimento?\n",
      "\n",
      "E, Mario, come stai procedendo con la tua esperienza di sviluppatore Python?\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)...\n",
      "--- Turno 7 ---\n",
      "üë§ Mi piace anche il calcio.\n",
      "ü§ñ Il calcio! √à un sport molto popolare in Italia e nel mondo.\n",
      "\n",
      "Sembra che tu abbia una vita molto varia, Mario! Da sviluppatore Python a cucinare la pasta e amare il calcio... ci sono molti interessi diversi che possono ispirarti in modo creativo!\n",
      "\n",
      "Forse potresti trovare un modo per integrare il tuo amore per il calcio nel progetto del chatbot? Ad esempio, creando una conversazione che simula un dialogo con un commentatore di calcio o che fornisce informazioni sulle partite e i giocatori...\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)...\n",
      "--- Turno 8 ---\n",
      "üë§ A che punto siamo?\n",
      "\n",
      "‚ö†Ô∏è LIMITE SUPERATO (8 messaggi). Aggiorno il summary...\n",
      "\n",
      "üìù Summary Aggiornato: * Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)\n",
      "* Interessi: LangChain, cucina, chatbot\n",
      "\n",
      "ü§ñ Siamo al primo passaggio della nostra conversazione!\n",
      "\n",
      "Abbiamo iniziato a discutere dei tuoi interessi e delle tue passioni, come la cucina, il calcio e il linguaggio di programmazione Python. Non abbiamo ancora creato un progetto specifico per il chatbot o non abbiamo ancora discusso del tuo lavoro come sviluppatore.\n",
      "\n",
      "Se vuoi, possiamo continuare a discutere e vedere dove ci porta la conversazione!\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)\n",
      "* Interessi: LangChain, cucina, chatbot...\n",
      "--- Turno 9 ---\n",
      "üë§ Come mi chiamo e dove lavoro?\n",
      "ü§ñ Sono felice di ricordarlo!\n",
      "\n",
      "Mi hai detto che ti chiami **Mario** e che lavori come **sviluppatore**. Inoltre, hai anche menzionato che vivi a **Palermo**, una citt√† meravigliosa in Sicilia!\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)\n",
      "* Interessi: LangChain, cucina, chatbot...\n",
      "--- Turno 10 ---\n",
      "üë§ Qual √® il mio framework preferito?\n",
      "ü§ñ Mi hai detto che il tuo framework di programmazione preferito √®... **Python**!\n",
      "\n",
      "In particolare, hai anche menzionato che hai 5 anni di esperienza con Python. Sembra che tu sia molto esperto in questo linguaggio!\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Linguaggio di programmazione: Python (5 anni di esperienza)\n",
      "* Interessi: LangChain, cucina, chatbot...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memoria Persistente su File\n",
    "\n",
    "Salviamo la memoria su file per persistenza tra sessioni usando ChatMessageHistory.\n",
    "\n",
    "Questo codice implementa la persistenza della memoria conversazionale su disco, permettendo di salvare e ricaricare la cronologia dei messaggi tra diverse sessioni.\n",
    "\n",
    "### Funzione di Salvataggio\n",
    "salva_memoria() converte l'oggetto InMemoryChatMessageHistory in formato JSON serializzabile. Itera su tutti i messaggi nella memoria, identifica il tipo (HumanMessage o AIMessage) e crea un dizionario con due campi: type (human/ai) e content (testo del messaggio). La lista di dizionari viene salvata in un file JSON con encoding UTF-8 e formattazione indentata per leggibilit√†.\n",
    "\n",
    "### Funzione di Caricamento\n",
    "carica_memoria() ricrea l'oggetto InMemoryChatMessageHistory dal file JSON. Prima controlla se il file esiste con os.path.exists(), poi legge il JSON e itera sui messaggi salvati. Per ogni messaggio, in base al campo type, aggiunge il contenuto alla memoria usando add_user_message() o add_ai_message() che creano automaticamente gli oggetti HumanMessage/AIMessage corretti.\n",
    "\n",
    "### Caso d'Uso Pratico\n",
    "Questo pattern √® essenziale per applicazioni reali dove vuoi che gli utenti riprendano le conversazioni dopo aver chiuso l'applicazione. Invece di perdere tutto il contesto conversazionale quando il programma termina, puoi salvare la memoria prima della chiusura e ricaricarla all'avvio successivo. Nel tuo sistema di summary memory, potresti salvare sia summary_store che summary_text_store per preservare anche i riassunti generati.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:09:26.059627945Z",
     "start_time": "2026-01-27T09:09:25.929330362Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import messages_to_dict, messages_from_dict, HumanMessage, AIMessage\n",
    "\n",
    "def salva_memoria(memory: InMemoryChatMessageHistory, filepath=\"memoria_chat.json\"):\n",
    "    # Converte tutti i messaggi in dizionari standard LangChain\n",
    "    dicts = messages_to_dict(memory.messages)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dicts, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Memoria salvata (formato nativo) su {filepath}\")\n",
    "\n",
    "def carica_memoria(filepath=\"memoria_chat.json\") -> InMemoryChatMessageHistory:\n",
    "    memory = InMemoryChatMessageHistory()\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # Ricostruisce automaticamente gli oggetti corretti (Human, AI, System, Tool, etc.)\n",
    "            messages = messages_from_dict(data)\n",
    "            memory.add_messages(messages)\n",
    "        print(f\"‚úÖ Memoria caricata ({len(messages)} messaggi)\")\n",
    "    return memory\n",
    "\n",
    "# TEST\n",
    "memoria = InMemoryChatMessageHistory()\n",
    "memoria.add_user_message(\"Ciao\")\n",
    "memoria.add_ai_message(\"Ciao! Come posso aiutarti?\")\n",
    "\n",
    "# Salvataggio\n",
    "salva_memoria(memoria)\n",
    "\n",
    "# Caricamento\n",
    "nuova_memoria = carica_memoria()\n",
    "print(nuova_memoria.messages)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memoria salvata (formato nativo) su memoria_chat.json\n",
      "‚úÖ Memoria caricata (2 messaggi)\n",
      "[HumanMessage(content='Ciao', additional_kwargs={}, response_metadata={}), AIMessage(content='Ciao! Come posso aiutarti?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Esempio Pratico: Chatbot con Memoria Persistente\n",
    "\n",
    "Creiamo un chatbot che ricorda informazioni tra sessioni usando LCEL e persistenza su file.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T09:12:46.468237223Z",
     "start_time": "2026-01-27T09:12:02.292626808Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# Assumendo che le funzioni salva_memoria e carica_memoria siano definite nelle celle precedenti\n",
    "\n",
    "class ChatbotConMemoria:\n",
    "    \"\"\"Chatbot con memoria persistente su JSON usando LCEL\"\"\"\n",
    "\n",
    "    def __init__(self, llm, user_id=\"default\", system_prompt=\"Sei un assistente utile.\"):\n",
    "        self.user_id = user_id\n",
    "        # Nome file dinamico basato sull'ID utente\n",
    "        self.memory_file = f\"memoria_{user_id}.json\"\n",
    "        self.llm = llm\n",
    "\n",
    "        # 1. Carica memoria (o ne crea una nuova)\n",
    "        self.memory = self._carica_memoria()\n",
    "\n",
    "        # 2. Crea prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "\n",
    "        # 3. Crea chain base\n",
    "        chain = prompt | self.llm\n",
    "\n",
    "        # 4. Avvolge la chain con la gestione della memoria\n",
    "        # Nota: Qui 'session_id' √® richiesto dalla firma di RunnableWithMessageHistory,\n",
    "        # ma noi forziamo l'uso di 'self.memory' perch√© questa classe gestisce un solo utente alla volta.\n",
    "        self.chain_with_history = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            get_session_history=lambda session_id: self.memory,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\"\n",
    "        )\n",
    "\n",
    "    def _carica_memoria(self) -> InMemoryChatMessageHistory:\n",
    "        \"\"\"Carica da file se esiste, altrimenti nuova memoria vuota\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            print(f\"üìÇ Memoria trovata per utente: {self.user_id}\")\n",
    "            # Qui usiamo la tua funzione definita in precedenza\n",
    "            return carica_memoria(self.memory_file)\n",
    "        else:\n",
    "            return InMemoryChatMessageHistory()\n",
    "\n",
    "    def _salva_memoria(self):\n",
    "        \"\"\"Salva lo stato attuale della memoria su file\"\"\"\n",
    "        # Qui usiamo la tua funzione definita in precedenza\n",
    "        salva_memoria(self.memory, self.memory_file)\n",
    "\n",
    "    def chat(self, messaggio):\n",
    "        \"\"\"Invia messaggio, ottieni risposta e salva\"\"\"\n",
    "        # Il session_id qui √® 'dummy' perch√© forziamo self.memory nel costruttore,\n",
    "        # ma LangChain lo richiede comunque nella config.\n",
    "        risposta = self.chain_with_history.invoke(\n",
    "            {\"input\": messaggio},\n",
    "            config={\"configurable\": {\"session_id\": self.user_id}}\n",
    "        )\n",
    "\n",
    "        # Salvataggio automatico dopo ogni interazione\n",
    "        self._salva_memoria()\n",
    "\n",
    "        return risposta.content\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Cancella memoria e file\"\"\"\n",
    "        self.memory.clear()\n",
    "        if os.path.exists(self.memory_file):\n",
    "            os.remove(self.memory_file)\n",
    "        print(f\"üóëÔ∏è Memoria resettata per {self.user_id}\")\n",
    "\n",
    "# --- ESECUZIONE TEST ---\n",
    "\n",
    "# Definizione LLM (assicurati di averlo definito, es: OpenAI o ChatOllama)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(\"=== Test Chatbot Persistent ===\\n\")\n",
    "\n",
    "# Sessione 1\n",
    "bot_mario = ChatbotConMemoria(llm, user_id=\"mario\", system_prompt=\"Sei un assistente siciliano simpatico.\")\n",
    "print(f\"Bot: {bot_mario.chat('Ciao, sono Mario e adoro le arancine!')}\")\n",
    "\n",
    "# Sessione 2 (Simulazione riavvio script)\n",
    "print(\"\\n--- Simulazione riavvio script ---\\n\")\n",
    "bot_mario_bis = ChatbotConMemoria(llm, user_id=\"mario\") # Ricarica lo stesso file\n",
    "print(f\"Bot: {bot_mario_bis.chat('Cosa mi piace mangiare?')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Chatbot Persistent ===\n",
      "\n",
      "‚úÖ Memoria salvata (formato nativo) su memoria_mario.json\n",
      "Bot: Ciao Mario!\n",
      "\n",
      "Sono felice di conoscerti! Sono un assistente siciliano, quindi non posso evitarti di dirti che le arancine sono una vera delizia siciliana! √à come se il sole e la terra della nostra terra si mescolassero in un piatto perfetto.\n",
      "\n",
      "Hai mai provato a farele da solo? Sono pi√π facili di quanto pensi, e ti dar√≤ la ricetta segreta per farti le migliori arancine del regno!\n",
      "\n",
      "E, come stai, Mario? Stai cercando qualcosa o vuoi parlare un po' di arancine?\n",
      "\n",
      "--- Simulazione riavvio script ---\n",
      "\n",
      "üìÇ Memoria trovata per utente: mario\n",
      "‚úÖ Memoria caricata (2 messaggi)\n",
      "‚úÖ Memoria salvata (formato nativo) su memoria_mario.json\n",
      "Bot: Mario! Sembra che tu abbia una gran passione per il cibo!\n",
      "\n",
      "Come sai, a Palazzo, la tua casa, ci sono molti piatti deliziosi. Ma se dovessi scegliere qualcosa di speciale, direi che ti piace molto mangiare... le arancine! √à come se fossero un po' di sole in un piatto!\n",
      "\n",
      "Ma, se non ti dispiace, anche il pizza e la pasta alla norma sono piatti molto popolari a Palazzo. E, naturalmente, non possiamo dimenticare i famosi \"funghi\" del re, che sono una specialit√† della nostra terra.\n",
      "\n",
      "E, come assistente, posso dirti che ci sono moltissimi altri piatti deliziosi da scoprire e provare. Se vuoi, posso darti alcune ricette o consigli per scoprire nuovi sapori!\n",
      "\n",
      "Quindi, Mario, cosa ti piace di pi√π mangiare? Le arancine, la pizza, la pasta... o forse qualcos'altro?\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Esercizio: Memoria Persistente con Summarization\n",
    "\n",
    "### Obiettivo\n",
    "Avete imparato a creare un chatbot che salva la conversazione su file JSON (persistenza) e uno che riassume i vecchi messaggi quando la memoria diventa troppo piena (summarization).\n",
    "\n",
    "Ora dovete unire queste due funzionalit√† in un unico script. Il vostro compito √® creare una classe o uno script che gestisca un chatbot capace di mantenere una conversazione infinita, ottimizzando i token tramite riassunto, e che sia in grado di \"spegnersi\" e \"riaccendersi\" senza perdere n√© i messaggi recenti n√© il riassunto accumulato.\n",
    "\n",
    "### Requisiti Tecnici\n",
    "Integrazione:\n",
    " - lo script deve utilizzare la logica di Summarization (aggiornamento del contesto quando si supera un limite N di messaggi) e salvare il tutto su disco.\n",
    " - Il File JSON: il salvataggio non riguarda pi√π solo una lista di messaggi. Dovete salvare (e ricaricare) lo stato completo della memoria.\n",
    " - Suggerimento: riflettete su come salvare il \"testo del riassunto\" attuale insieme ai \"messaggi recenti\".\n",
    "\n",
    "HINT: per testare il funzionamento, impostate il limite di messaggi nel buffer a un numero basso (es. 4 o 6 messaggi) in modo che il riassunto scatti velocemente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Note e Best Practices\n",
    "\n",
    "### Cosa abbiamo imparato:\n",
    "1. **ChatMessageHistory**: Memorizzare tutti i messaggi usando l'API moderna di LangChain 1.0+\n",
    "2. **RunnableWithMessageHistory**: Gestire automaticamente la memoria nelle chain\n",
    "3. **Conversation Summary**: Riassumere i messaggi per gestire conversazioni lunghe e risparmiare token\n",
    "4. **Memoria persistente**: Salvare su file per mantenere contesto tra le sessioni dell'utente\n",
    "\n",
    "### Quando usare cosa:\n",
    "- **ChatMessageHistory**: per brevi conversazioni (< 10 turni), prototipi, sviluppo\n",
    "- **Conversation Summary**: per conversazioni lunghe (> 20 turni), produzione\n",
    "- **Memoria persistente**: Quando serve ricordare tra sessioni o riavvii\n",
    "\n",
    "\n",
    "### Prossimi passi:\n",
    "- Aggiungeremo RAG per knowledge base\n",
    "- Integreremo memoria con Agent e Tools\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulazioni! Hai completato il Notebook 3! üéâ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

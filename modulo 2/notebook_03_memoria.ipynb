{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Memoria Conversazionale con LCEL\n",
    "\n",
    "**Obiettivo**: Implementare memoria breve e a lungo termine per un chatbot con LangChain 1.0+ usando LCEL (LangChain Expression Language)\n",
    "\n",
    "**Nota**: Questo notebook usa l'API moderna LCEL di LangChain 1.0+, non le API legacy (ConversationChain, ConversationBufferMemory).\n",
    "\n",
    "---\n",
    "Questo snippet implementa un chatbot conversazionale con memoria usando LangChain e Ollama.\n",
    "Il codice crea un'istanza di ChatOllama che si connette al modello locale llama3.2:3b, poi utilizza un dizionario Python (store) per memorizzare le cronologie delle conversazioni separate per session_id.\n",
    "\n",
    "La funzione get_session_history gestisce il recupero o la creazione di nuove cronologie in memoria.\n",
    "Il wrapper RunnableWithMessageHistory integra automaticamente la gestione della memoria nel modello, permettendo di mantenere il contesto conversazionale tra chiamate successive.\n",
    "\n",
    "Nell'esempio pratico, il chatbot ricorda il nome dell'utente nella seconda domanda perch√© entrambe le interazioni utilizzano lo stesso session_id (\"abcd\"), consentendo al modello di accedere alla cronologia completa della conversazione memorizzata in memoria."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:51:59.808470790Z",
     "start_time": "2026-01-14T13:51:44.178054437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Usa ChatOllama direttamente senza prompt template custom\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.7)\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrappa direttamente il modello\n",
    "chat = RunnableWithMessageHistory(\n",
    "    llm,\n",
    "    get_session_history\n",
    ")\n",
    "\n",
    "# Usa\n",
    "response = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Ciao, mi chiamo Marco\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"abcd\"}}\n",
    ")\n",
    "print(response.content)\n",
    "\n",
    "response = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Qual √® il mio nome?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"abcd\"}}\n",
    ")\n",
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao Marco! Sono felice di conoscerti. Come posso aiutarti oggi? Vuoi parlare di qualcosa in particolare o vuoi semplicemente chiacchierare un po'?\n",
      "Mi dispiace, ma non sono sicuro del tuo nome. Hai solo detto \"Ciao Marco\" all'inizio della nostra conversazione, quindi non so se sia il tuo vero nome o semplicemente un saluto. Se vuoi, puoi dirmi il tuo nome reale!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ChatMessageHistory - Memoria Breve Termine\n",
    "\n",
    "In LCEL, usiamo `InMemoryChatMessageHistory` per memorizzare tutti i messaggi della conversazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 1. Inizializza il modello LLM locale\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.1)\n",
    "\n",
    "# 2. Crea lo store per memorizzare le sessioni\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Recupera o crea una nuova cronologia per la sessione\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 3. Crea il chatbot con memoria\n",
    "chat = RunnableWithMessageHistory(\n",
    "    llm,\n",
    "    get_session_history\n",
    ")\n",
    "\n",
    "# 4. SESSIONE STUDENTE 1\n",
    "print(\"=== CONVERSAZIONE STUDENTE 1 ===\")\n",
    "response1 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Ciao, mi chiamo Luca e studio a Milano\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_1\"}}\n",
    ")\n",
    "print(f\"AI: {response1.content}\\n\")\n",
    "\n",
    "response2 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Dove studio io?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_1\"}}\n",
    ")\n",
    "print(f\"AI: {response2.content}\\n\")\n",
    "\n",
    "# 5. SESSIONE STUDENTE 2 (sessione diversa, memoria separata)\n",
    "print(\"=== CONVERSAZIONE STUDENTE 2 ===\")\n",
    "response3 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Ciao, mi chiamo Sara e studio a Roma\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_2\"}}\n",
    ")\n",
    "print(f\"AI: {response3.content}\\n\")\n",
    "\n",
    "response4 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Dove studio io?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_2\"}}\n",
    ")\n",
    "print(f\"AI: {response4.content}\\n\")\n",
    "\n",
    "# 6. Torniamo alla SESSIONE STUDENTE 1\n",
    "print(\"=== TORNIAMO ALLO STUDENTE 1 ===\")\n",
    "response5 = chat.invoke(\n",
    "    [{\"role\": \"user\", \"content\": \"Come mi chiamo?\"}],\n",
    "    config={\"configurable\": {\"session_id\": \"studente_1\"}}\n",
    ")\n",
    "print(f\"AI: {response5.content}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chat con Memoria Buffer usando LCEL\n",
    "\n",
    "Questo pattern rappresenta l'implementazione production-ready di un chatbot conversazionale con memoria utilizzando l'architettura LCEL (LangChain Expression Language). √à il metodo standard per costruire assistenti AI professionali con gestione della cronologia conversazionale.\n",
    "\n",
    "### Architettura a Tre Livelli\n",
    "Il codice separa chiaramente tre responsabilit√†. Il ChatPromptTemplate definisce la struttura dell'interazione, includendo un messaggio di sistema per configurare il comportamento dell'assistente, un MessagesPlaceholder per iniettare dinamicamente la cronologia conversazionale e un template per l'input utente. La chain LCEL (prompt | llm) crea una pipeline che processa sequenzialmente il prompt template e lo invia al modello. Il wrapper RunnableWithMessageHistory orchestra automaticamente il recupero della cronologia, l'integrazione nel prompt e il salvataggio dei nuovi messaggi.\n",
    "\n",
    "### Gestione Avanzata degli Input\n",
    "I parametri input_messages_key e history_messages_key sono necessari perch√© il runnable accetta un dizionario come input anzich√© una semplice lista. Questi specificano dove mappare l'input dell'utente e dove iniettare la cronologia nel template, consentendo a LangChain di gestire automaticamente il flusso dei dati tra memoria e modello.\n",
    "\n",
    "### Vantaggi del Pattern\n",
    "Questo approccio garantisce modularit√† e manutenibilit√†. Puoi modificare il comportamento del sistema cambiando solo il prompt template, aggiungere preprocessing/postprocessing estendendo la chain, o sostituire il backend di memoria senza riscrivere la logica applicativa. Il sistema supporta nativamente sessioni multiple isolate attraverso session_id, rendendolo ideale per applicazioni multi-utente.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Crea prompt template con LCEL (ChatPromptTemplate)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sei un assistente amichevole. Rispondi alle domande dell'utente.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Placeholder per la storia\n",
    "    (\"human\", \"{input}\")  # Input dell'utente\n",
    "])\n",
    "\n",
    "# Crea chain LCEL: prompt | llm\n",
    "chain = prompt | llm\n",
    "\n",
    "# Store per gestire memoria per diverse sessioni\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    \"\"\"Funzione per ottenere/creare memoria per una sessione\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Crea RunnableWithMessageHistory (gestisce automaticamente la memoria)\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Test conversazione (LCEL usa invoke con config)\n",
    "session_id = \"test_session\"\n",
    "\n",
    "print(\"=== Conversazione 1 ===\")\n",
    "risposta1 = conversation.invoke(\n",
    "    {\"input\": \"Ciao, mi chiamo Mario\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Risposta: {risposta1.content}\\n\")\n",
    "\n",
    "print(\"=== Conversazione 2 ===\")\n",
    "risposta2 = conversation.invoke(\n",
    "    {\"input\": \"Qual √® il mio nome?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Risposta: {risposta2.content}\\n\")\n",
    "\n",
    "print(\"=== Conversazione 3 ===\")\n",
    "risposta3 = conversation.invoke(\n",
    "    {\"input\": \"Dove lavoro?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Risposta: {risposta3.content}\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flusso di Esecuzione\n",
    "Quando si invoca `conversation.invoke({\"input\": \"Ciao, mi chiamo Mario\"}, config={\"configurable\": {\"session_id\": \"test_session\"}})`, succede questo in sequenza:\n",
    "  - LangChain chiama get_session_history(\"test_session\") per recuperare la cronologia\n",
    "  - sostituisce {input} nel template con \"Ciao, mi chiamo Mario\", inserisce i messaggi storici nel MessagesPlaceholder\n",
    "  - invia tutto al modello tramite la chain\n",
    "  - salva automaticamente sia il messaggio utente che la risposta AI nella memoria.\n",
    "\n",
    "\n",
    "\n",
    "Nella seconda invocazione, quando chiedi \"Qual √® il mio nome?\", il sistema recupera la stessa cronologia contenente il messaggio precedente, quindi il modello pu√≤ rispondere correttamente \"Mario\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat con Summary Memory\n",
    "\n",
    "Usiamo summary memory in una conversazione lunga.\n",
    "\n",
    "Questo codice implementa un sistema di summary memory incrementale che aggiorna progressivamente il riassunto della conversazione invece di ricrearlo da zero ogni volta.\n",
    "\n",
    "### Logica del Summary Incrementale\n",
    "Quando la conversazione supera 6 messaggi, il sistema recupera il summary precedente da `summary_text_store` e lo combina con i nuovi messaggi da archiviare.\n",
    "Il prompt di aggiornamento chiede esplicitamente all'LLM di \"combinare le vecchie info con le nuove\", mantenendo i dati storici (nome, citt√† iniziali) e aggiungendo i nuovi dettagli (hobby, interessi emersi dopo).\n",
    "\n",
    "### Vantaggi dell'Approccio\n",
    "Questo metodo preserva meglio le informazioni iniziali attraverso molteplici cicli di summarization. Il summary diventa cumulativo: ogni volta che viene aggiornato, porta con s√© i fatti delle conversazioni precedenti.\n",
    "\n",
    "### Flusso di Esecuzione\n",
    "Il sistema conta i messaggi regolari escludendo il messaggio [CONTEXT] che contiene il summary. Quando supera il limite, mantiene gli ultimi 2 messaggi recenti per continuit√† e invia all'LLM sia il vecchio summary che i nuovi messaggi da archiviare. L'LLM genera un summary aggiornato che viene salvato in `summary_text_store` e iniettato come primo HumanMessage nella nuova cronologia.\n",
    "\n",
    "La temperature ridotta a 0.1 garantisce risposte pi√π deterministiche e accurate nel recupero delle informazioni.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:59:41.725953986Z",
     "start_time": "2026-01-14T13:56:19.828600133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Inizializza LLM\n",
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.1)\n",
    "\n",
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Sei un assistente amichevole.\n",
    "IMPORTANTE: Leggi attentamente TUTTA la cronologia.\n",
    "Se c'√® un messaggio di [CONTEXT] o [RIASSUNTO], usalo come verit√† assoluta sull'utente.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# Store\n",
    "summary_store = {}\n",
    "summary_text_store = {}\n",
    "\n",
    "def get_summary_history(session_id: str, max_messages=6) -> BaseChatMessageHistory:\n",
    "    \"\"\"Crea/recupera cronologia con summary INCREMENTALE\"\"\"\n",
    "\n",
    "    if session_id not in summary_store:\n",
    "        summary_store[session_id] = InMemoryChatMessageHistory()\n",
    "        summary_text_store[session_id] = \"\" # Inizializza stringa vuota\n",
    "\n",
    "    history = summary_store[session_id]\n",
    "\n",
    "    # 1. Identifica i messaggi \"veri\" (escludendo il messaggio tecnico di context se presente)\n",
    "    regular_messages = [m for m in history.messages\n",
    "                       if not (isinstance(m, HumanMessage) and m.content.startswith(\"[CONTEXT]\"))]\n",
    "\n",
    "    # 2. Controllo Limite\n",
    "    if len(regular_messages) > max_messages:\n",
    "        print(f\"\\n‚ö†Ô∏è LIMITE SUPERATO ({len(regular_messages)} messaggi). Aggiorno il summary...\\n\")\n",
    "\n",
    "        # Separiamo messaggi da archiviare (old) e messaggi da tenere (recent)\n",
    "        # Teniamo gli ultimi 2 per mantenere il flusso della conversazione fluido\n",
    "        messages_to_summarize = regular_messages[:-2]\n",
    "        recent_messages = regular_messages[-2:]\n",
    "\n",
    "        # Recuperiamo il VECCHIO summary (se esiste)\n",
    "        previous_summary = summary_text_store.get(session_id, \"\")\n",
    "\n",
    "        # Convertiamo i messaggi da archiviare in testo\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{'UTENTE' if isinstance(m, HumanMessage) else 'AI'}: {m.content}\"\n",
    "            for m in messages_to_summarize\n",
    "        ])\n",
    "\n",
    "        # 3. Prompt di AGGIORNAMENTO (Cruciale: Unisce Vecchio + Nuovo)\n",
    "        update_prompt = f\"\"\"Sei un gestore di memoria.\n",
    "Ecco le informazioni che gi√† conosci sull'utente:\n",
    "{previous_summary}\n",
    "\n",
    "Ecco la nuova conversazione appena avvenuta:\n",
    "{conversation_text}\n",
    "\n",
    "COMPITO:\n",
    "Aggiorna il profilo utente combinando le vecchie info con le nuove.\n",
    "Mantieni nome, citt√†, lavoro e aggiungi nuovi interessi o dettagli.\n",
    "Sii sintetico. Rispondi SOLO con la lista dei fatti aggiornata.\n",
    "\"\"\"\n",
    "\n",
    "        # Genera il nuovo summary\n",
    "        summary_response = llm.invoke([{\"role\": \"user\", \"content\": update_prompt}])\n",
    "        new_summary_text = summary_response.content.strip()\n",
    "\n",
    "        # Salviamo il testo aggiornato\n",
    "        summary_text_store[session_id] = new_summary_text\n",
    "        print(f\"üìù Summary Aggiornato: {new_summary_text}\\n\")\n",
    "\n",
    "        # 4. Ricostruzione della History\n",
    "        new_history = InMemoryChatMessageHistory()\n",
    "\n",
    "        # Iniettiamo il summary come PRIMO messaggio (chiaro per l'LLM)\n",
    "        # Usiamo un formato esplicito\n",
    "        context_message = HumanMessage(content=f\"[CONTEXT] RIEPILOGO DATI UTENTE:\\n{new_summary_text}\")\n",
    "        new_history.add_message(context_message)\n",
    "\n",
    "        # Reinseriamo gli ultimi messaggi per non perdere il filo immediato\n",
    "        for msg in recent_messages:\n",
    "            new_history.add_message(msg)\n",
    "\n",
    "        # Sostituiamo la history nello store\n",
    "        summary_store[session_id] = new_history\n",
    "        return new_history\n",
    "\n",
    "    return history\n",
    "\n",
    "# Setup Runnable\n",
    "conversation_summary = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_summary_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# --- TEST ---\n",
    "print(\"=== TEST SUMMARY INCREMENTALE ===\\n\")\n",
    "session_id = \"test_incrementale_v2\"\n",
    "\n",
    "# Simuliamo una conversazione lunga per forzare il summary\n",
    "messaggi = [\n",
    "    \"Ciao, mi chiamo Mario.\",                       # 1\n",
    "    \"Lavoro a Palermo come sviluppatore.\",          # 2\n",
    "    \"Ho 5 anni di esperienza in Python.\",           # 3\n",
    "    \"Sto studiando LangChain.\",                     # 4\n",
    "    \"Mi piace cucinare la pasta.\",                  # 5\n",
    "    \"Voglio fare un chatbot.\",                      # 6\n",
    "    \"Mi piace anche il calcio.\",                    # 7\n",
    "    \"A che punto siamo?\",                           # 8\n",
    "    \"Come mi chiamo e dove lavoro?\",                # 9\n",
    "    \"Qual √® il mio framework preferito?\",           # 10\n",
    "]\n",
    "\n",
    "# messaggi = [\n",
    "#     \"Ciao, mi chiamo Giulia.\",\n",
    "#     \"Attualmente abito a Milano.\",\n",
    "#     \"Lavoro come Data Analyst in una banca.\",\n",
    "#     \"Uso Python e SQL tutti i giorni per lavoro.\",\n",
    "#     \"Nel tempo libero mi piace molto fare giardinaggio.\",\n",
    "#     \"Ho un cane di nome Rex.\",\n",
    "#     \"Il mio piatto preferito √® la carbonara.\",\n",
    "#     \"Vorrei imparare a usare LangChain per automatizzare dei report.\",\n",
    "#     \"Qual √® la differenza principale tra LangChain e LlamaIndex?\",\n",
    "#     \"Sto avendo qualche difficolt√† a capire come funzionano gli agenti.\",\n",
    "#     \"Ieri sono andata al cinema a vedere un film di fantascienza.\",\n",
    "#     \"Tra l'altro, il mio colore preferito √® il verde.\",\n",
    "#     \"Ho una notizia: tra un mese mi trasferir√≤ a Torino.\",\n",
    "#     \"L√¨ continuer√≤ a lavorare ma in smart working.\",\n",
    "#     \"Vorrei anche iscrivermi a un corso di tennis.\",\n",
    "#     \"Non bevo caff√®, preferisco il t√® matcha.\",\n",
    "#     \"Mi consigli un buon libro tecnico sull'AI?\",\n",
    "#     \"Sto provando a far girare un modello locale sul mio PC.\",\n",
    "#     \"Senti, facciamo un punto della situazione.\",\n",
    "#     \"Ti ricordi come mi chiamo, dove vivo ora e dove andr√≤ a vivere tra poco?\"\n",
    "# ]\n",
    "\n",
    "for i, msg in enumerate(messaggi, 1):\n",
    "    print(f\"--- Turno {i} ---\")\n",
    "    print(f\"üë§ {msg}\")\n",
    "\n",
    "    risposta = conversation_summary.invoke(\n",
    "        {\"input\": msg},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    print(f\"ü§ñ {risposta.content}\\n\")\n",
    "\n",
    "    # Debug Memoria\n",
    "    history = summary_store[session_id].messages\n",
    "    if len(history) > 0 and \"[CONTEXT]\" in history[0].content:\n",
    "        print(f\"üîç Contenuto Memoria (Primo Messaggio): {history[0].content}...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SUMMARY INCREMENTALE ===\n",
      "\n",
      "--- Turno 1 ---\n",
      "üë§ Ciao, mi chiamo Mario.\n",
      "ü§ñ Ciao Mario! Sono felice di conoscerti! Come posso aiutarti oggi?\n",
      "\n",
      "--- Turno 2 ---\n",
      "üë§ Lavoro a Palermo come sviluppatore.\n",
      "ü§ñ Ciao Mario! Sembra che tu sia un professionista molto specifico, lavorando come sviluppatore a Palermo. Quale tipo di progetti stai lavorando attualmente? Ecco, posso aiutarti con qualsiasi domanda o problema tecnico che possa avere.\n",
      "\n",
      "--- Turno 3 ---\n",
      "üë§ Ho 5 anni di esperienza in Python.\n",
      "ü§ñ Hai una solida base di conoscenza in Python! 5 anni di esperienza sono un ottimo risultato, dimostra la tua capacit√† di apprendimento e adattamento nel campo della programmazione.\n",
      "\n",
      "Quale tipo di progetti hai lavorato fino a ora? Sono stati pi√π progetti personali o hai avuto l'opportunit√† di lavorare su progetti aziendali?\n",
      "\n",
      "--- Turno 4 ---\n",
      "üë§ Sto studiando LangChain.\n",
      "ü§ñ LangChain √® un framework molto interessante per la programmazione naturale e la gestione delle interazioni con i dati. √à una tecnologia in continua evoluzione che offre molte possibilit√† per lo sviluppo di applicazioni intelligenti.\n",
      "\n",
      "Studiare LangChain significa che stai cercando di approfondire le tue conoscenze sulla programmazione naturale, la gestione delle interazioni con i dati e l'intelligenza artificiale. Spero che tu possa trovare il corso o la risorsa giusta per aiutarti a comprendere meglio questo framework.\n",
      "\n",
      "Sei gi√† stato in contatto con la comunit√† di LangChain o hai trovato qualcuno con cui discutere le tue domande e sfide?\n",
      "\n",
      "--- Turno 5 ---\n",
      "üë§ Mi piace cucinare la pasta.\n",
      "\n",
      "‚ö†Ô∏è LIMITE SUPERATO (8 messaggi). Aggiorno il summary...\n",
      "\n",
      "üìù Summary Aggiornato: * Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Esperienza in Python: 5 anni\n",
      "* Interessi: Non specificati\n",
      "\n",
      "ü§ñ Cucinare la pasta √® un'attivit√† molto piacevole! √à una tradizione culinaria molto diffusa in Italia, dove sei cresciuto a Palermo.\n",
      "\n",
      "Ci sono cos√¨ tante ricette diverse per preparare la pasta, non solo la classica spaghetti al pomodoro. Cosa ti piace cucinare di pi√π? Hai una ricetta preferita o vuoi provare qualcosa di nuovo?\n",
      "\n",
      "Forse potresti condividere con me la tua esperienza culinaria e io posso aiutarti a trovare nuove idee per le tue prossime cene.\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Esperienza in Python: 5 anni\n",
      "* Interessi: Non specificati...\n",
      "--- Turno 6 ---\n",
      "üë§ Voglio fare un chatbot.\n",
      "ü§ñ Un chatbot! Sembra un progetto interessante!\n",
      "\n",
      "Studiando LangChain, hai probabilmente gi√† iniziato a esplorare la possibilit√† di creare un chatbot che possa interagire con gli utenti in modo naturale.\n",
      "\n",
      "Cosa ti piacerebbe fare con il tuo chatbot? Vuoi crearlo per una specifica applicazione, come ad esempio un assistente virtuale o un sistema di supporto al cliente? O semplicemente vuoi esplorare le possibilit√† della programmazione naturale e dell'intelligenza artificiale?\n",
      "\n",
      "Sono qui per aiutarti a pianificare il tuo progetto e fornirti consigli e risorse per aiutarti a creare un chatbot efficace.\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Esperienza in Python: 5 anni\n",
      "* Interessi: Non specificati...\n",
      "--- Turno 7 ---\n",
      "üë§ Mi piace anche il calcio.\n",
      "ü§ñ Il calcio! √à uno sport molto popolare in Italia, dove sei cresciuto a Palermo.\n",
      "\n",
      "Sembra che tu abbia una passione per diverse attivit√†, come la programmazione con LangChain e la cucina della pasta, nonch√© lo sport del calcio. Forse hai anche un team di calcio preferito o un giocatore che ti ispira?\n",
      "\n",
      "Se vuoi parlare pi√π a lungo dei tuoi interessi o delle tue passioni, sono qui per ascoltarti!\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Esperienza in Python: 5 anni\n",
      "* Interessi: Non specificati...\n",
      "--- Turno 8 ---\n",
      "üë§ A che punto siamo?\n",
      "\n",
      "‚ö†Ô∏è LIMITE SUPERATO (8 messaggi). Aggiorno il summary...\n",
      "\n",
      "üìù Summary Aggiornato: * Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Esperienza in Python: 5 anni\n",
      "* Interessi:\n",
      " + LangChain\n",
      " + Programmazione naturale\n",
      " + Intelligenza artificiale\n",
      " + Cucinare la pasta\n",
      "\n",
      "ü§ñ Siamo al primo passaggio della nostra conversazione. Ti ho presentato i dati relativi alla tua identit√† e ai tuoi interessi. Se vuoi aggiornarmi o cambiare argomento, sono qui per ascoltarti!\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Esperienza in Python: 5 anni\n",
      "* Interessi:\n",
      " + LangChain\n",
      " + Programmazione naturale\n",
      " + Intelligenza artificiale\n",
      " + Cucinare la pasta...\n",
      "--- Turno 9 ---\n",
      "üë§ Come mi chiamo e dove lavoro?Qual √® il mio framework preferito?\n",
      "ü§ñ Sembra che tu abbia gi√† fornito queste informazioni in precedenza!\n",
      "\n",
      "Ti ho detto che ti chiami Mario e lavori a Palermo. E, se ricordo bene, hai anche menzionato LangChain come tuo framework preferito per la programmazione!\n",
      "\n",
      "üîç Contenuto Memoria (Primo Messaggio): [CONTEXT] RIEPILOGO DATI UTENTE:\n",
      "* Nome: Mario\n",
      "* Citt√†: Palermo\n",
      "* Lavoro: Sviluppatore\n",
      "* Esperienza in Python: 5 anni\n",
      "* Interessi:\n",
      " + LangChain\n",
      " + Programmazione naturale\n",
      " + Intelligenza artificiale\n",
      " + Cucinare la pasta...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memoria Persistente su File\n",
    "\n",
    "Salviamo la memoria su file per persistenza tra sessioni usando ChatMessageHistory.\n",
    "\n",
    "Questo codice implementa la persistenza della memoria conversazionale su disco, permettendo di salvare e ricaricare la cronologia dei messaggi tra diverse sessioni.\n",
    "\n",
    "### Funzione di Salvataggio\n",
    "salva_memoria() converte l'oggetto InMemoryChatMessageHistory in formato JSON serializzabile. Itera su tutti i messaggi nella memoria, identifica il tipo (HumanMessage o AIMessage) e crea un dizionario con due campi: type (human/ai) e content (testo del messaggio). La lista di dizionari viene salvata in un file JSON con encoding UTF-8 e formattazione indentata per leggibilit√†.\n",
    "\n",
    "### Funzione di Caricamento\n",
    "carica_memoria() ricrea l'oggetto InMemoryChatMessageHistory dal file JSON. Prima controlla se il file esiste con os.path.exists(), poi legge il JSON e itera sui messaggi salvati. Per ogni messaggio, in base al campo type, aggiunge il contenuto alla memoria usando add_user_message() o add_ai_message() che creano automaticamente gli oggetti HumanMessage/AIMessage corretti.\n",
    "\n",
    "### Caso d'Uso Pratico\n",
    "Questo pattern √® essenziale per applicazioni reali dove vuoi che gli utenti riprendano le conversazioni dopo aver chiuso l'applicazione. Invece di perdere tutto il contesto conversazionale quando il programma termina, puoi salvare la memoria prima della chiusura e ricaricarla all'avvio successivo. Nel tuo sistema di summary memory, potresti salvare sia summary_store che summary_text_store per preservare anche i riassunti generati.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import messages_to_dict, messages_from_dict, HumanMessage, AIMessage\n",
    "\n",
    "def salva_memoria(memory: InMemoryChatMessageHistory, filepath=\"memoria_chat.json\"):\n",
    "    # Converte tutti i messaggi in dizionari standard LangChain\n",
    "    dicts = messages_to_dict(memory.messages)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dicts, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Memoria salvata (formato nativo) su {filepath}\")\n",
    "\n",
    "def carica_memoria(filepath=\"memoria_chat.json\") -> InMemoryChatMessageHistory:\n",
    "    memory = InMemoryChatMessageHistory()\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # Ricostruisce automaticamente gli oggetti corretti (Human, AI, System, Tool, etc.)\n",
    "            messages = messages_from_dict(data)\n",
    "            memory.add_messages(messages)\n",
    "        print(f\"‚úÖ Memoria caricata ({len(messages)} messaggi)\")\n",
    "    return memory\n",
    "\n",
    "# TEST\n",
    "memoria = InMemoryChatMessageHistory()\n",
    "memoria.add_user_message(\"Ciao\")\n",
    "memoria.add_ai_message(\"Ciao! Come posso aiutarti?\")\n",
    "\n",
    "# Salvataggio\n",
    "salva_memoria(memoria)\n",
    "\n",
    "# Caricamento\n",
    "nuova_memoria = carica_memoria()\n",
    "print(nuova_memoria.messages)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Esempio Pratico: Chatbot con Memoria Persistente\n",
    "\n",
    "Creiamo un chatbot che ricorda informazioni tra sessioni usando LCEL e persistenza su file.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# Assumendo che le funzioni salva_memoria e carica_memoria siano definite nelle celle precedenti\n",
    "\n",
    "class ChatbotConMemoria:\n",
    "    \"\"\"Chatbot con memoria persistente su JSON usando LCEL\"\"\"\n",
    "\n",
    "    def __init__(self, llm, user_id=\"default\", system_prompt=\"Sei un assistente utile.\"):\n",
    "        self.user_id = user_id\n",
    "        # Nome file dinamico basato sull'ID utente\n",
    "        self.memory_file = f\"memoria_{user_id}.json\"\n",
    "        self.llm = llm\n",
    "\n",
    "        # 1. Carica memoria (o ne crea una nuova)\n",
    "        self.memory = self._carica_memoria()\n",
    "\n",
    "        # 2. Crea prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "\n",
    "        # 3. Crea chain base\n",
    "        chain = prompt | self.llm\n",
    "\n",
    "        # 4. Avvolge la chain con la gestione della memoria\n",
    "        # Nota: Qui 'session_id' √® richiesto dalla firma di RunnableWithMessageHistory,\n",
    "        # ma noi forziamo l'uso di 'self.memory' perch√© questa classe gestisce un solo utente alla volta.\n",
    "        self.chain_with_history = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            get_session_history=lambda session_id: self.memory,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"history\"\n",
    "        )\n",
    "\n",
    "    def _carica_memoria(self) -> InMemoryChatMessageHistory:\n",
    "        \"\"\"Carica da file se esiste, altrimenti nuova memoria vuota\"\"\"\n",
    "        if os.path.exists(self.memory_file):\n",
    "            print(f\"üìÇ Memoria trovata per utente: {self.user_id}\")\n",
    "            # Qui usiamo la tua funzione definita in precedenza\n",
    "            return carica_memoria(self.memory_file)\n",
    "        else:\n",
    "            return InMemoryChatMessageHistory()\n",
    "\n",
    "    def _salva_memoria(self):\n",
    "        \"\"\"Salva lo stato attuale della memoria su file\"\"\"\n",
    "        # Qui usiamo la tua funzione definita in precedenza\n",
    "        salva_memoria(self.memory, self.memory_file)\n",
    "\n",
    "    def chat(self, messaggio):\n",
    "        \"\"\"Invia messaggio, ottieni risposta e salva\"\"\"\n",
    "        # Il session_id qui √® 'dummy' perch√© forziamo self.memory nel costruttore,\n",
    "        # ma LangChain lo richiede comunque nella config.\n",
    "        risposta = self.chain_with_history.invoke(\n",
    "            {\"input\": messaggio},\n",
    "            config={\"configurable\": {\"session_id\": self.user_id}}\n",
    "        )\n",
    "\n",
    "        # Salvataggio automatico dopo ogni interazione\n",
    "        self._salva_memoria()\n",
    "\n",
    "        return risposta.content\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Cancella memoria e file\"\"\"\n",
    "        self.memory.clear()\n",
    "        if os.path.exists(self.memory_file):\n",
    "            os.remove(self.memory_file)\n",
    "        print(f\"üóëÔ∏è Memoria resettata per {self.user_id}\")\n",
    "\n",
    "# --- ESECUZIONE TEST ---\n",
    "\n",
    "# Definizione LLM (assicurati di averlo definito, es: OpenAI o ChatOllama)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(\"=== Test Chatbot Persistent ===\\n\")\n",
    "\n",
    "# Sessione 1\n",
    "bot_mario = ChatbotConMemoria(llm, user_id=\"mario\", system_prompt=\"Sei un assistente siciliano simpatico.\")\n",
    "print(f\"Bot: {bot_mario.chat('Ciao, sono Mario e adoro le arancine!')}\")\n",
    "\n",
    "# Sessione 2 (Simulazione riavvio script)\n",
    "print(\"\\n--- Simulazione riavvio script ---\\n\")\n",
    "bot_mario_bis = ChatbotConMemoria(llm, user_id=\"mario\") # Ricarica lo stesso file\n",
    "print(f\"Bot: {bot_mario_bis.chat('Cosa mi piace mangiare?')}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Esercizio: Memoria Persistente con Summarization\n",
    "\n",
    "### Obiettivo\n",
    "Avete imparato a creare un chatbot che salva la conversazione su file JSON (persistenza) e uno che riassume i vecchi messaggi quando la memoria diventa troppo piena (summarization).\n",
    "\n",
    "Ora dovete unire queste due funzionalit√† in un unico script. Il vostro compito √® creare una classe o uno script che gestisca un chatbot capace di mantenere una conversazione infinita, ottimizzando i token tramite riassunto, e che sia in grado di \"spegnersi\" e \"riaccendersi\" senza perdere n√© i messaggi recenti n√© il riassunto accumulato.\n",
    "\n",
    "### Requisiti Tecnici\n",
    "Integrazione:\n",
    " - lo script deve utilizzare la logica di Summarization (aggiornamento del contesto quando si supera un limite N di messaggi) e salvare il tutto su disco.\n",
    " - Il File JSON: il salvataggio non riguarda pi√π solo una lista di messaggi. Dovete salvare (e ricaricare) lo stato completo della memoria.\n",
    " - Suggerimento: riflettete su come salvare il \"testo del riassunto\" attuale insieme ai \"messaggi recenti\".\n",
    "\n",
    "HINT: per testare il funzionamento, impostate il limite di messaggi nel buffer a un numero basso (es. 4 o 6 messaggi) in modo che il riassunto scatti velocemente.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Note e Best Practices\n",
    "\n",
    "### Cosa abbiamo imparato:\n",
    "1. **ChatMessageHistory**: Memorizzare tutti i messaggi usando l'API moderna di LangChain 1.0+\n",
    "2. **RunnableWithMessageHistory**: Gestire automaticamente la memoria nelle chain\n",
    "3. **Conversation Summary**: Riassumere i messaggi per gestire conversazioni lunghe e risparmiare token\n",
    "4. **Memoria persistente**: Salvare su file per mantenere contesto tra le sessioni dell'utente\n",
    "\n",
    "**LCEL (LangChain Expression Language)**: usare un approccio moderno e flessibile per creare chain (langchain 1.0+)\n",
    "\n",
    "### Quando usare cosa:\n",
    "- **ChatMessageHistory**: per brevi conversazioni (< 10 turni), prototipi, sviluppo\n",
    "- **Conversation Summary**: per conversazioni lunghe (> 20 turni), produzione\n",
    "- **Memoria persistente**: Quando serve ricordare tra sessioni o riavvii\n",
    "\n",
    "\n",
    "### Prossimi passi:\n",
    "- Aggiungeremo RAG per knowledge base\n",
    "- Integreremo memoria con Agent e Tools\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulazioni! Hai completato il Notebook 3! üéâ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

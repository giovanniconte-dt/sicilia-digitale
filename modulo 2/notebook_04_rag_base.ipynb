{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Introduzione a RAG - Embedding e Retrieval Base\n",
    "\n",
    "Implementare i componenti base di RAG: embedding, semantic search e retrieval semplice\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Import\n",
    "\n",
    "Importiamo le librerie necessarie e configuriamo l'ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model=\"llama3.2:3b\"\n",
    "embedder = \"all-minilm:l6-v2\" # nomic-embed-text\n",
    "\n",
    "# Inizializza LLM (chat model per generazione)\n",
    "llm = ChatOllama(\n",
    "    model=model,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Inizializza Embeddings (per rappresentazione vettoriale)\n",
    "# Usa Ollama con modello embedding locale\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=embedder\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup completato!\")\n",
    "print(f\"LLM: llama3.2:3b\")\n",
    "print(f\"Embeddings: {embedder}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cos'√® un Embedding?\n",
    "\n",
    "Creiamo embedding di testi per vedere come funziona la rappresentazione vettoriale.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Testi di esempio\n",
    "testi = [\n",
    "    \"Quando posso ritirare il documento d'identit√†?\",\n",
    "    \"Quando posso ritirare la carta d'identit√†?\",\n",
    "    \"Quali sono gli orari di apertura del comune?\",\n",
    "    \"Quali sono gli orari del comune?\",\n",
    "    \"Come si cucina la pasta?\",\n",
    "    \"Ricetta per pasta carbonara\"\n",
    "]\n",
    "\n",
    "# Crea embedding per ogni testo\n",
    "print(\"=== Creazione Embedding ===\\n\")\n",
    "embeddings_list = []\n",
    "\n",
    "for i, testo in enumerate(testi):\n",
    "    embedding = embeddings.embed_query(testo)\n",
    "    embeddings_list.append(embedding)\n",
    "    print(f\"Testo {i+1}: {testo[:50]}...\")\n",
    "    print(f\"Embedding: [{embedding[0]:.4f}, {embedding[1]:.4f}, ..., {embedding[-1]:.4f}]\")\n",
    "    print(f\"Dimensione: {len(embedding)} dimensioni\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Similarit√† Semantica\n",
    "\n",
    "Calcoliamo la similarit√† tra testi usando cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calcola cosine similarity tra due vettori\"\"\"\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "# Confronta similarit√† tra testi\n",
    "print(\"=== Similarit√† Semantica (Cosine Similarity) ===\\n\")\n",
    "print(\"Simili (stesso argomento):\")\n",
    "for i in [0, 1]:  # Testi simili: ritiro documento\n",
    "    for j in [0, 1]:\n",
    "        if i < j:\n",
    "            sim = cosine_similarity(embeddings_list[i], embeddings_list[j])\n",
    "            print(f\"  '{testi[i][:40]}...' vs '{testi[j][:40]}...'\")\n",
    "            print(f\"  Similarit√†: {sim:.4f}\\n\")\n",
    "\n",
    "print(\"Diversi (argomento diverso):\")\n",
    "for i in [0, 4]:  # Testo documento vs testo pasta\n",
    "    for j in [0, 4]:\n",
    "        if i != j:\n",
    "            sim = cosine_similarity(embeddings_list[i], embeddings_list[j])\n",
    "            print(f\"  '{testi[i][:40]}...' vs '{testi[j][:40]}...'\")\n",
    "            print(f\"  Similarit√†: {sim:.4f}\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge Base Semplice\n",
    "\n",
    "Creiamo una knowledge base di esempio (documenti FAQ) e creiamo embedding per ogni documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Knowledge base di esempio: FAQ su pratiche comunali\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"titolo\": \"Ritiro Documenti\",\n",
    "        \"contenuto\": \"Il documento d'identit√† pu√≤ essere ritirato dal luned√¨ al venerd√¨ dalle 9:00 alle 13:00 presso l'ufficio anagrafe. √à necessario presentare la ricevuta del pagamento e un documento di riconoscimento valido.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"titolo\": \"Orari Uffici\",\n",
    "        \"contenuto\": \"Gli uffici comunali sono aperti dal luned√¨ al venerd√¨ dalle 9:00 alle 13:00 e il marted√¨ e gioved√¨ anche il pomeriggio dalle 15:00 alle 17:30. Chiusi sabato, domenica e giorni festivi.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"titolo\": \"Certificati Online\",\n",
    "        \"contenuto\": \"√à possibile richiedere certificati online tramite il portale SPID o CIE. I certificati vengono emessi entro 3 giorni lavorativi e inviati via email o PEC.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"titolo\": \"Anagrafe - Cambio Residenza\",\n",
    "        \"contenuto\": \"Per il cambio di residenza √® necessario presentare domanda all'ufficio anagrafe entro 20 giorni dal trasferimento. Servono: documento d'identit√†, codice fiscale e contratto di affitto o propriet√†.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"titolo\": \"Tassa Rifiuti - ISEE\",\n",
    "        \"contenuto\": \"Per richiedere l'esenzione o riduzione TARI in base all'ISEE, √® necessario presentare il modello ISEE presso l'ufficio tributi entro il 31 gennaio di ogni anno.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Crea embedding per ogni documento nella knowledge base\n",
    "print(\"=== Creazione Knowledge Base con Embedding ===\\n\")\n",
    "kb_embeddings = []\n",
    "\n",
    "for doc in knowledge_base:\n",
    "    embedding = embeddings.embed_query(doc[\"contenuto\"])\n",
    "    kb_embeddings.append({\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"titolo\": doc[\"titolo\"],\n",
    "        \"contenuto\": doc[\"contenuto\"],\n",
    "        \"embedding\": embedding\n",
    "    })\n",
    "    print(f\"‚úÖ Documento {doc['id']}: {doc['titolo']}\")\n",
    "    print(f\"   Embedding: {len(embedding)} dimensioni\\n\")\n",
    "\n",
    "print(f\"Knowledge base pronta con {len(kb_embeddings)} documenti!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Search Base\n",
    "\n",
    "Implementiamo semantic search per trovare i documenti pi√π rilevanti per una query.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def semantic_search(query, kb_embeddings_list, top_k=3):\n",
    "    \"\"\"\n",
    "    Cerca i documenti pi√π simili alla query usando semantic search\n",
    "    \n",
    "    Args:\n",
    "        query: Testo della query\n",
    "        kb_embeddings_list: Lista di documenti con embedding\n",
    "        top_k: Numero di risultati da restituire\n",
    "    \n",
    "    Returns:\n",
    "        Lista di documenti pi√π simili con score\n",
    "    \"\"\"\n",
    "    # Crea embedding della query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    # Calcola similarit√† con tutti i documenti\n",
    "    similarities = []\n",
    "    for doc in kb_embeddings_list:\n",
    "        sim = cosine_similarity(query_embedding, doc[\"embedding\"])\n",
    "        similarities.append({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"titolo\": doc[\"titolo\"],\n",
    "            \"contenuto\": doc[\"contenuto\"],\n",
    "            \"similarity\": sim\n",
    "        })\n",
    "    \n",
    "    # Ordina per similarit√† (decrescente)\n",
    "    similarities.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    \n",
    "    # Restituisci top-k\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test semantic search\n",
    "print(\"=== Semantic Search - Test ===\\n\")\n",
    "\n",
    "query1 = \"Quando posso ritirare la carta d'identit√†?\"\n",
    "print(f\"Query 1: {query1}\\n\")\n",
    "risultati1 = semantic_search(query1, kb_embeddings, top_k=2)\n",
    "\n",
    "for i, risultato in enumerate(risultati1, 1):\n",
    "    print(f\"Risultato {i}:\")\n",
    "    print(f\"  Titolo: {risultato['titolo']}\")\n",
    "    print(f\"  Similarit√†: {risultato['similarity']:.4f}\")\n",
    "    print(f\"  Contenuto: {risultato['contenuto'][:100]}...\\n\")\n",
    "\n",
    "print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "query2 = \"Quali sono gli orari del comune?\"\n",
    "print(f\"Query 2: {query2}\\n\")\n",
    "risultati2 = semantic_search(query2, kb_embeddings, top_k=2)\n",
    "\n",
    "for i, risultato in enumerate(risultati2, 1):\n",
    "    print(f\"Risultato {i}:\")\n",
    "    print(f\"  Titolo: {risultato['titolo']}\")\n",
    "    print(f\"  Similarit√†: {risultato['similarity']:.4f}\")\n",
    "    print(f\"  Contenuto: {risultato['contenuto'][:150]}...\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Semplice\n",
    "\n",
    "Combiniamo retrieval e generation per creare un sistema RAG base.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def rag_simple(query, kb_embeddings_list, llm, top_k=3):\n",
    "    \"\"\"\n",
    "    Sistema RAG semplice: retrieval + generation\n",
    "    \n",
    "    Args:\n",
    "        query: Domanda dell'utente\n",
    "        kb_embeddings_list: Knowledge base con embedding\n",
    "        llm: Modello LLM per generazione\n",
    "        top_k: Numero di documenti da recuperare\n",
    "    \n",
    "    Returns:\n",
    "        Risposta generata con fonti\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieval - trova documenti rilevanti\n",
    "    documenti_rilevanti = semantic_search(query, kb_embeddings_list, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Costruisci contesto dai documenti\n",
    "    contesto = \"\\n\\n\".join([\n",
    "        f\"Documento {doc['id']} - {doc['titolo']}:\\n{doc['contenuto']}\"\n",
    "        for doc in documenti_rilevanti\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Costruisci prompt con contesto\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Sei un assistente che risponde alle domande basandoti SOLO sul contesto fornito.\n",
    "Se la risposta non √® nel contesto, d√¨ 'Non ho informazioni sufficienti nel contesto fornito.'\n",
    "Cita sempre il numero del documento quando possibile.\"\"\"),\n",
    "        (\"human\", \"\"\"Contesto:\n",
    "{context}\n",
    "\n",
    "Domanda: {query}\n",
    "\n",
    "Risposta:\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Step 4: Genera risposta\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    risposta = chain.invoke({\"context\": contesto, \"query\": query})\n",
    "    \n",
    "    # Step 5: Restituisci risposta con fonti\n",
    "    fonti = [{\"id\": doc[\"id\"], \"titolo\": doc[\"titolo\"]} for doc in documenti_rilevanti]\n",
    "    \n",
    "    return {\n",
    "        \"risposta\": risposta,\n",
    "        \"fonti\": fonti\n",
    "    }\n",
    "\n",
    "# Test RAG\n",
    "print(\"=== RAG Semplice - Test ===\\n\")\n",
    "\n",
    "query = \"Quando posso ritirare il documento d'identit√†?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "risultato = rag_simple(query, kb_embeddings, llm, top_k=2)\n",
    "\n",
    "print(f\"Risposta: {risultato['risposta']}\\n\")\n",
    "print(f\"Fonti:\")\n",
    "for fonte in risultato['fonti']:\n",
    "    print(f\"  - Documento {fonte['id']}: {fonte['titolo']}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test RAG con Diverse Query\n",
    "\n",
    "Testiamo il sistema RAG con diverse domande per vedere come funziona.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test con diverse query\n",
    "query_test = [\n",
    "    \"Quali sono gli orari di apertura degli uffici?\",\n",
    "    \"Come posso richiedere un certificato online?\",\n",
    "    \"Cosa serve per il cambio di residenza?\",\n",
    "    \"Come funziona l'esenzione TARI con ISEE?\"\n",
    "]\n",
    "\n",
    "print(\"=== Test RAG con Diverse Query ===\\n\")\n",
    "\n",
    "for i, query in enumerate(query_test, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query {i}: {query}\\n\")\n",
    "    \n",
    "    risultato = rag_simple(query, kb_embeddings, llm, top_k=2)\n",
    "    \n",
    "    print(f\"Risposta: {risultato['risposta']}\\n\")\n",
    "    fonti_unite = ', '.join([f\"Doc {f['id']}\" for f in risultato['fonti']])\n",
    "    print(f\"Fonti: {fonti_unite}\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Note e Best Practices\n",
    "\n",
    "### Cosa abbiamo imparato:\n",
    "1. **Embedding**: rappresentazione vettoriale del testo che cattura significato semantico\n",
    "2. **Similarit√† semantica**: testi simili hanno embedding simili (cosine similarity)\n",
    "3. **Semantic Search**: ricerca dei documenti basata su significato\n",
    "4. **RAG Base**: Retrieval + Generation = sistema che risponde usando knowledge base\n",
    "\n",
    "### Limitazioni di questo approccio:\n",
    "- **Embedding in memoria**: Non scala a migliaia di documenti\n",
    "- **No vector database**: Ricerca lineare (lenta su grandi dataset)\n",
    "- **No chunking**: Usiamo documenti interi (non ottimale per documenti lunghi)\n",
    "- **No persistenza**: Embedding vengono ricalcolati ogni volta\n",
    "\n",
    "### Best Practices:\n",
    "- **Dimensioni embedding**: 384-768 dimensioni √® un buon compromesso\n",
    "- **Top-k retrieval**: 3-5 documenti sono solitamente sufficienti\n",
    "- **Prompt engineering**: Istruzioni chiare sul contesto riducono hallucination\n",
    "- **Test query**: Valutare con query reali, non solo esempi\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulazioni! Hai completato il Notebook 4! üéâ**\n",
    "\n",
    "Nel prossimo notebook vedremo come usare Chroma (vector database) e chunking avanzato per creare un sistema RAG completo e scalabile.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
